# Тестовое задание А2
## Допущения

Так как задание было сформулировано не очень строго, я следал следующие допущения:
- Новые записи на сайте https://www.lesegais.ru/open-area/deal добавляются в начало таблицы (это следует дефолтной сортировке по колонке "дата сделки")
- Новые записи о сделке не могут быть датированы раньше, чем уже существующие записи
- Утилита работает с данными, размещенными только по этой ссылке https://www.lesegais.ru/open-area/deal

## Запуск решения

- При первом запуске необходимо раскомментировать строчку в Program.cs для скачивания браузера. При повторном запуске этого делать не нужно.
- Также необходимо отредактировать поле connectionString в DBManager.cs в соответствии с настройками вашей базы данных.

## Пояснение решения

Для скраппинга страницы используется библиотека PuppeteerSharp в связке с headless браузером на движке Chrome. Это необходимо для корректного получения данных со страницы https://www.lesegais.ru/open-area/deal, так как сам сайт написан на React, и требует поддержку JS, которой нет при отправке стандартных HTTP-запросов.

В коде используется много конструкций async/await потому, что библиотека PuppeteerSharp содержит в себе асинхронные методы. Однако в контексте асинхронного метода Main, который тоже поддерживает конструкцию await решение по сути становится синхронным, как и требовалось в задании.

Также при использовании методов этой библиотеки выставлены задержки, это нужно, чтобы браузер успел полностью прогрузить данные на странице.

Взаимодествие с базой данных Sql Server происхоит посредством встроенных средств языка.
Файл со структурой базы данных лежит внутри проекта. В нем же есть дамп первых 100 строк.

Чтобы постоянно не обращаться к базе данных и снизить нагрузку на нее, в утилите реализован кэш, который хранит в себе хеш-коды последних 100 записей из базы данных. При скраппинге новых записей с сайта, они сначала проверяются на вхождение в кэш и только потом попадают в базу данных.

## Трудозатраты 

Ранее мне не приходилось сталкиваться с задачей веб-скраппинга и работой с MS Sql Server. На изучение инструментов для решения задания я потратил около 2 часов. Суммарно выполение задания заняло у меня около 5 часов (включая время, на изучение различных способов решения).




